{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "concate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0yrKo_hJk70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f8652063-316d-40f0-f275-7b78b3317546"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8FojmZYKFuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "737cfd73-f31c-4e47-c108-94586820c37c"
      },
      "source": [
        "%cd '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/'\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1-WHOINXFZ4JSjL7g5g092rZ8rCZfdBPw/odir-ivashnyov/ODIR\n",
            "CrossValidationScore.py\t\t  Predictions.py\n",
            "cv_split.pickle\t\t\t  predict.py\n",
            "data\t\t\t\t  __pycache__\n",
            "EDA.ipynb\t\t\t  README.md\n",
            "initial_test_b3_fold3_clahe\t  split_folds.ipynb\n",
            "logs\t\t\t\t  submit\n",
            "logs_final\t\t\t  submit_final\n",
            "MNIST\t\t\t\t  submit_initial_test_b3.csv\n",
            "model_concate_10_no_epoch.txt\t  submit_oanh_initial_test_b3.csv\n",
            "model_concate_15epoch_weight.txt  submit_vessel_initial_test_b3.csv\n",
            "model_concate_19epoch.txt\t  train_1cycle.py\n",
            "model_concate_20epoch.txt\t  train_all_folds_args.sh\n",
            "model_concate_5_no_epoch.txt\t  train_all_folds.sh\n",
            "model_concate_6epoch.txt\t  train.py\n",
            "model_concate_epoch_20.txt\t  utils.py\n",
            "odir_submit.py\t\t\t  Visualization.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suTAV-P-KI2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catalyst==20.2.4\n",
        "!pip install tqdm==4.33\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install pytorch_toolbelt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEO8HqWoJ99O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "355d61ef-6897-4646-ee33-2061ba7e7734"
      },
      "source": [
        "from sklearn import svm\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.callbacks import LambdaCallback\n",
        "from utils import *\n",
        "#from simple_model import *\n",
        "#from preprocess import *\n",
        "#from make_labels import *\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import collections\n",
        "from pytorch_toolbelt.inference import tta\n",
        "from catalyst.dl.callbacks import InferCallback\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "from torch.nn.functional import softmax\n",
        "from catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback, F1ScoreCallback, ConfusionMatrixCallback, MixupCallback\n",
        "from catalyst.contrib.nn.schedulers import OneCycleLR, ReduceLROnPlateau, StepLR, MultiStepLR\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import pickle\n",
        "import time\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "alchemy not available, to install alchemy, run `pip install alchemy-catalyst`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgaC5cJJzVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\n",
        "    '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/splited_train_new.csv')\n",
        "splits = pickle.load(open(\n",
        "    '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/cv_split.pickle', 'rb'))\n",
        "labels = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "image_size = 240\n",
        "fold_idx = 3\n",
        "batch_size = 1\n",
        "lr = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-tHRoJ_K3Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path1 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/clahe-train/'\n",
        "valid_path1 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/clahe-train/'\n",
        "\n",
        "train_dataset1 = EyeDataset(dataset_path=train_path1,\n",
        "                           labels=data.loc[splits['train_idx'][fold_idx],labels].values,\n",
        "                           ids=data.loc[splits['train_idx'][fold_idx],labels].values,\n",
        "                           albumentations_tr=aug_train_heavy(image_size))\n",
        "\n",
        "train_loader1 = DataLoader(train_dataset1,\n",
        "                          num_workers=8,\n",
        "                          pin_memory=False,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "logdir = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs_final/initial_test_b3_fold3_clahe/'\n",
        "modelA = prepare_model('efficientnet-b3', 8)\n",
        "modelA.cuda()\n",
        "modelA.load_state_dict(torch.load(os.path.join(\n",
        "    logdir, '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs_final/initial_test_b3_fold3_clahe/checkpoints/best.pth'))['model_state_dict'])\n",
        "modelA.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx1pf7D4LDnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path2 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/vessel-train-full/'\n",
        "valid_path2 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/vessel-train-full/'\n",
        "\n",
        "train_dataset2 = EyeDataset(dataset_path=train_path2,\n",
        "                           labels=data.loc[splits['train_idx'][fold_idx],labels].values,\n",
        "                           ids=data.loc[splits['train_idx'][fold_idx],labels].values,\n",
        "                           albumentations_tr=aug_train_heavy(image_size))\n",
        "train_loader2 = DataLoader(train_dataset2,\n",
        "                          num_workers=8,\n",
        "                          pin_memory=False,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "logdir = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs_final/initial_test_b3_fold3_vessel/'\n",
        "modelB = prepare_model('efficientnet-b3', 8)\n",
        "modelB.cuda()\n",
        "modelB.load_state_dict(torch.load(os.path.join(\n",
        "    logdir, '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs_final/initial_test_b3_fold3_vessel/checkpoints/best.pth'))['model_state_dict'])\n",
        "modelB.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTDYTy-Vh3LX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "2accef49-6c28-41c9-8cfc-43d925179f6d"
      },
      "source": [
        "for i2 in train_loader2:\n",
        "  input2,target2 = i2\n",
        "  #print(input1)\n",
        "  #print(target1)\n",
        "  print(i2)\n",
        "  input2 = input2.cuda()\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f5c229b82eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#print(input1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#print(target1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/utils.py\", line 75, in __getitem__\n    image = cv2.imread(os.path.join(self.dataset_path, self.ids[index]))\n  File \"/usr/lib/python3.6/posixpath.py\", line 94, in join\n    genericpath._check_arg_types('join', a, *p)\n  File \"/usr/lib/python3.6/genericpath.py\", line 149, in _check_arg_types\n    (funcname, s.__class__.__name__)) from None\nTypeError: join() argument must be str or bytes, not 'ndarray'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lfXYDnVrjFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, modelA, modelB, nb_classes=8):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.out2 = nn.Linear(1536, 8)\n",
        "        self.modelA.fc = nn.Identity()\n",
        "        self.modelB.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.modelA.extract_features(x1)\n",
        "        x2 = self.modelB.extract_features(x2)\n",
        "        \n",
        "        x1 = nn.AdaptiveAvgPool2d((1, 1))(x1)\n",
        "        x2 = nn.AdaptiveAvgPool2d((1, 1))(x2)\n",
        "        x1 = x1.view(1,1536)\n",
        "        x1 = torch.Tensor.cpu(x1).detach().numpy()\n",
        "        x2 = x2.view(1,1536)\n",
        "        x2 = torch.Tensor.cpu(x2).detach().numpy()\n",
        "        x = x1+x2        \n",
        "        sigmoid = tf.math.sigmoid(x)\n",
        "        multi = tf.math.multiply(sigmoid,x2)\n",
        "        add = multi + x1\n",
        "        add = add.numpy()\n",
        "        x = torch.from_numpy(add)\n",
        "        x = self.drop(x)\n",
        "        x = x.flatten()\n",
        "        x = self.out2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PawOjQiZxgqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probabilities = []\n",
        "x_model = NeuralNet(modelA, modelB)\n",
        "y = x_model(input1,input2)\n",
        "print(y)\n",
        "print(type(y))\n",
        "y = torch.Tensor.cpu(y).detach().numpy()\n",
        "print(y)\n",
        "print(type(y))\n",
        "probabilities = softmax(torch.from_numpy(y),dim=0).numpy()\n",
        "print(probabilities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MKM4WJHL0AJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "b6ce7762-0d92-4729-ed39-3cd98bf857ba"
      },
      "source": [
        "optimizer = torch.optim.Adam(x_model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.75, patience=2)\n",
        "loss_log = []\n",
        "PATH_SAVE = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs_final/clahe_vessel_forgetgate/'\n",
        "if not os.path.exists(PATH_SAVE):\n",
        "       os.makedirs(PATH_SAVE)\n",
        "for epoch in range(20):\n",
        "  print('====Epoch ',epoch,\": \")\n",
        "  for i1, i2 in zip(enumerate(train_loader1),enumerate(train_loader2)):\n",
        "    #print(i1)\n",
        "    stt1, data1 = i1\n",
        "    input1,target1 = data1\n",
        "    #print(i2)\n",
        "    stt2, data2 = i2\n",
        "    input2,target2 = data2\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    input1 = input1.cuda()\n",
        "    input2 = input2.cuda()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = x_model(input1,input2)\n",
        "    #print(outputs)\n",
        "    #print(target1)\n",
        "    outputs = outputs.view(1,8)\n",
        "\n",
        "    loss = criterion(outputs, target1)\n",
        "    loss.backward()\n",
        "    #print(loss)\n",
        "    optimizer.step()\n",
        "  loss_log.append(loss)\n",
        "  if (epoch>=15):\n",
        "    torch.save(x_model.state_dict(), PATH_SAVE + 'clahe_vessel_forgetgate_epoch_'+str(epoch)+'.pth')\n",
        "\n",
        "with open(PATH_SAVE+\"clahe_vessel_forgetgate.txt\", \"w\") as output:\n",
        "    output.write(str(loss_log))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====Epoch  0 : \n",
            "tensor(5.2391, grad_fn=<NllLossBackward>)\n",
            "====Epoch  1 : \n",
            "tensor(7.0463, grad_fn=<NllLossBackward>)\n",
            "====Epoch  2 : \n",
            "tensor(6.8624, grad_fn=<NllLossBackward>)\n",
            "====Epoch  3 : \n",
            "tensor(6.9787, grad_fn=<NllLossBackward>)\n",
            "====Epoch  4 : \n",
            "tensor(6.8653, grad_fn=<NllLossBackward>)\n",
            "====Epoch  5 : \n",
            "tensor(4.8871, grad_fn=<NllLossBackward>)\n",
            "====Epoch  6 : \n",
            "tensor(5.2329, grad_fn=<NllLossBackward>)\n",
            "====Epoch  7 : \n",
            "tensor(3.8114, grad_fn=<NllLossBackward>)\n",
            "====Epoch  8 : \n",
            "tensor(8.5890, grad_fn=<NllLossBackward>)\n",
            "====Epoch  9 : \n",
            "tensor(4.5288, grad_fn=<NllLossBackward>)\n",
            "====Epoch  10 : \n",
            "tensor(5.3942, grad_fn=<NllLossBackward>)\n",
            "====Epoch  11 : \n",
            "tensor(5.5714, grad_fn=<NllLossBackward>)\n",
            "====Epoch  12 : \n",
            "tensor(8.2257, grad_fn=<NllLossBackward>)\n",
            "====Epoch  13 : \n",
            "tensor(3.5530, grad_fn=<NllLossBackward>)\n",
            "====Epoch  14 : \n",
            "tensor(6.2300, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mn_quBW_KlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "80804fff-ee68-4d6b-88a5-5c223bdb1d04"
      },
      "source": [
        "x_model.load_state_dict(torch.load(os.path.join(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs_final/clahe_vessel_forgetgate/clahe_vessel_forgetgate_epoch_19.pth')))\n",
        "x_model.eval()\n",
        "\n",
        "test_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/XYZ_ODIR.csv')\n",
        "test_data_left = test_data.copy()\n",
        "test_data_right = test_data.copy()\n",
        "test_data_left.loc[:, 'id'] = test_data_left.ID.apply(\n",
        "    lambda x: str(x)+'_left.jpg')\n",
        "test_data_right.loc[:, 'id'] = test_data_left.ID.apply(\n",
        "    lambda x: str(x)+'_right.jpg')\n",
        "test_data = pd.concat([test_data_left, test_data_right])\n",
        "test_data.sort_values(['ID'], inplace=True)\n",
        "test_path = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/clahe-test/'\n",
        "test_dataset = EyeDataset(dataset_path=test_path,\n",
        "                          labels=test_data.loc[:, labels].values,\n",
        "                          ids=test_data.loc[:, 'id'].values,\n",
        "                          albumentations_tr=aug_val(image_size))\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         num_workers=8,\n",
        "                         pin_memory=False,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False)\n",
        "\n",
        "\n",
        "test_path1 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/vessel-test-full/'\n",
        "test_dataset_vessel = EyeDataset(dataset_path=test_path1,\n",
        "                                 labels=test_data.loc[:, labels].values,\n",
        "                                 ids=test_data.loc[:, 'id'].values,\n",
        "                                 albumentations_tr=aug_val(image_size))\n",
        "test_loader_vessel = DataLoader(test_dataset_vessel,\n",
        "                                num_workers=8,\n",
        "                                pin_memory=False,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False)\n",
        "\n",
        "probabilities = []\n",
        "probabilities_list = []\n",
        "exp_name = 'initial_test_b3'\n",
        "optimizer = torch.optim.Adam(x_model.parameters(), lr=1e-5)\n",
        "\n",
        "for i1, i2 in zip(enumerate(test_loader), enumerate(test_loader_vessel)):\n",
        "    # print(i1)\n",
        "    stt1, data1 = i1\n",
        "    input1, target1 = data1\n",
        "    # print(i2)\n",
        "    stt2, data2 = i2\n",
        "    input2, target2 = data2\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    input1 = input1.cuda()\n",
        "    input2 = input2.cuda()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = x_model(input1, input2)\n",
        "    outputs = torch.Tensor.cpu(outputs).detach().numpy()\n",
        "    probabilities = softmax(torch.from_numpy(outputs), dim=0).numpy()\n",
        "    # print(probabilities)\n",
        "    for idx in range(probabilities.shape[0]):\n",
        "        if all(probabilities[:] < 0.5):\n",
        "            probabilities[0] = 1.0\n",
        "    probabilities_list.append(probabilities)\n",
        "arr_list = []\n",
        "arr = np.array(probabilities_list)\n",
        "arr_list.append(arr)\n",
        "print(arr_list)\n",
        "\n",
        "\n",
        "probabilities_combined = np.stack(arr_list, axis=0).mean(axis=0)\n",
        "predicted_labels = pd.DataFrame(probabilities_combined, columns=labels)\n",
        "predicted_labels['id'] = test_data.loc[:, 'id'].values\n",
        "predicted_labels.loc[:, 'ID'] = predicted_labels.id.apply(\n",
        "    lambda x: x.split('_')[0])\n",
        "predicted_labels_groupped = predicted_labels.groupby(\n",
        "    ['ID']).aggregate(dict(zip(labels, ['max']*(len(labels)))))\n",
        "print(type(predicted_labels_groupped))\n",
        "predicted_labels_groupped['ID'] = predicted_labels_groupped.index.values.astype(\n",
        "    int)\n",
        "predicted_labels_groupped.reset_index(drop=True, inplace=True)\n",
        "predicted_labels_groupped.sort_values('ID', inplace=True)\n",
        "predicted_labels_groupped = predicted_labels_groupped.loc[:, ['ID']+labels]\n",
        "predicted_labels_groupped.to_csv(\n",
        "    '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/submit_final/clahe_vessel_forgetgate_epoch_19.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-48aa5d5345a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m x_model.load_state_dict(torch.load(os.path.join(\n\u001b[0m\u001b[1;32m      2\u001b[0m         '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/concate_2model_clahe_vessel/concate_2model_clahe_vessel_epoch_5.pth')))\n\u001b[1;32m      3\u001b[0m \u001b[0mx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/XYZ_ODIR.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyi3lLGz7fIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}