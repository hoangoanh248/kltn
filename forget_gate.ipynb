{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forget_gate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy22ifdrUG8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2c283add-2608-44d6-da6f-85d80fc056ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8o9V1lRUNL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "dc955496-3601-4bbc-8e14-ca55ec1d62b7"
      },
      "source": [
        "%cd '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/'\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR\n",
            "CrossValidationScore.py\t\t  Predictions.py\n",
            "cv_split.pickle\t\t\t  predict.py\n",
            "data\t\t\t\t  __pycache__\n",
            "EDA.ipynb\t\t\t  README.md\n",
            "initial_test_b3_fold3_clahe\t  split_folds.ipynb\n",
            "logs\t\t\t\t  submit\n",
            "MNIST\t\t\t\t  submit_initial_test_b3.csv\n",
            "model_concate_10_no_epoch.txt\t  submit_oanh_initial_test_b3.csv\n",
            "model_concate_15epoch_weight.txt  submit_vessel_initial_test_b3.csv\n",
            "model_concate_19epoch.txt\t  train_1cycle.py\n",
            "model_concate_20epoch.txt\t  train_all_folds_args.sh\n",
            "model_concate_5_no_epoch.txt\t  train_all_folds.sh\n",
            "model_concate_6epoch.txt\t  train.py\n",
            "model_concate_epoch_20.txt\t  utils.py\n",
            "odir_submit.py\t\t\t  Visualization.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVNQKaDIUXOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catalyst==20.2.4\n",
        "!pip install tqdm==4.33\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install pytorch_toolbelt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cml6R1UaBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "723de05f-18bf-4af2-c0ba-dcbf33644a60"
      },
      "source": [
        "from sklearn import svm\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.callbacks import LambdaCallback\n",
        "from utils import *\n",
        "#from simple_model import *\n",
        "#from preprocess import *\n",
        "#from make_labels import *\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import collections\n",
        "from pytorch_toolbelt.inference import tta\n",
        "from catalyst.dl.callbacks import InferCallback\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "from torch.nn.functional import softmax\n",
        "from catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback, F1ScoreCallback, ConfusionMatrixCallback, MixupCallback\n",
        "from catalyst.contrib.nn.schedulers import OneCycleLR, ReduceLROnPlateau, StepLR, MultiStepLR\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import pickle\n",
        "import time\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "alchemy not available, to install alchemy, run `pip install alchemy-catalyst`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsFLR0gf0mXV",
        "colab_type": "text"
      },
      "source": [
        "**prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1DeokJqUc13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\n",
        "    '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/splited_train.csv')\n",
        "splits = pickle.load(open(\n",
        "    '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/cv_split.pickle', 'rb'))\n",
        "labels = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "image_size = 240\n",
        "fold_idx = 3\n",
        "batch_size = 1\n",
        "lr = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXELHQwOUdSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path1 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/ODIR-5K_Training_Dataset/'\n",
        "valid_path1 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/ODIR-5K_Training_Dataset/'\n",
        "\n",
        "train_dataset1 = EyeDataset(dataset_path=train_path1,\n",
        "                           labels=data.loc[splits['train_idx'][fold_idx],labels].values,\n",
        "                           ids=data.loc[splits['train_idx'][fold_idx], 'id'].values,\n",
        "                           albumentations_tr=aug_train_heavy(image_size))\n",
        "\n",
        "# val_dataset1 = EyeDataset(dataset_path=valid_path1, \n",
        "# \t\t\t\t\t                labels=data.loc[splits['test_idx'][fold_idx],labels].values, \n",
        "# \t\t\t\t\t                ids=data.loc[splits['test_idx'][fold_idx],'id'].values, \n",
        "# \t\t\t\t\t                albumentations_tr=aug_val(image_size))\n",
        "# train_dataset1 = EyeDataset(dataset_path=train_path1,\n",
        "#                            labels=data.loc[:,labels].values,\n",
        "#                            ids=data.loc[:, 'id'].values,\n",
        "#                            albumentations_tr=aug_train_heavy(image_size))\n",
        "\n",
        "train_loader1 = DataLoader(train_dataset1,\n",
        "                          num_workers=8,\n",
        "                          pin_memory=False,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "\n",
        "logdir = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/initial_test_b3_fold3_org/'\n",
        "modelA = prepare_model('efficientnet-b3', 8)\n",
        "modelA.cuda()\n",
        "modelA.load_state_dict(torch.load(os.path.join(\n",
        "    logdir, '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/initial_test_b3_fold3_org/checkpoints/best.pth'))['model_state_dict'])\n",
        "modelA.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohb_YM5hUep-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path2 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/training-dataset/'\n",
        "valid_path2 = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/data/training-dataset/'\n",
        "\n",
        "# train_dataset2 = EyeDataset(dataset_path=train_path2,\n",
        "#                            labels=data.loc[splits['train_idx'][fold_idx], labels].values,\n",
        "#                            ids=data.loc[splits['train_idx'][fold_idx], 'id'].values,\n",
        "#                            albumentations_tr=aug_train_heavy(image_size))\n",
        "# val_dataset2 = EyeDataset(dataset_path=valid_path2, \n",
        "# \t\t\t\t\t                labels=data.loc[splits['test_idx'][fold_idx],labels].values, \n",
        "# \t\t\t\t\t                ids=data.loc[splits['test_idx'][fold_idx],'id'].values, \n",
        "# \t\t\t\t\t                albumentations_tr=aug_val(image_size))\n",
        "train_dataset2 = EyeDataset(dataset_path=train_path2,\n",
        "                           labels=data.loc[:, labels].values,\n",
        "                           ids=data.loc[:, 'id'].values,\n",
        "                           albumentations_tr=aug_train_heavy(image_size))\n",
        "\n",
        "train_loader2 = DataLoader(train_dataset2,\n",
        "                          num_workers=8,\n",
        "                          pin_memory=False,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "logdir = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/initial_test_b3_fold3_vessel/'\n",
        "modelB = prepare_model('efficientnet-b3', 8)\n",
        "modelB.cuda()\n",
        "modelB.load_state_dict(torch.load(os.path.join(\n",
        "    logdir, '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/initial_test_b3_fold3_vessel/checkpoints/best.pth'))['model_state_dict'])\n",
        "modelB.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mNdcaq0U9PS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "5c5cc386-395c-49f4-ff8a-fe0fd7a33952"
      },
      "source": [
        "for i1 in train_loader1:\n",
        "  input1,target1 = i1\n",
        "  #print(input1)\n",
        "  #print(target1)\n",
        "  print(i1)\n",
        "  input1 = input1.cuda()\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          ...,\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          ...,\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          ...,\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeN7JD7jYXfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "ca865958-30a3-4a35-dda5-f2e78a9749b6"
      },
      "source": [
        "for i2 in train_loader2:\n",
        "  input2,target2 = i2\n",
        "  #print(input2)\n",
        "  #print(target2)\n",
        "  input2 = input2.cuda()\n",
        "  print(i2)\n",
        "  break  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          ...,\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          ...,\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          ...,\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqQWETvl6wXH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**FORGET GATE 2 MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGbw2La2UgGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, modelA, modelB, nb_classes=8):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.out2 = nn.Linear(1536, 8)\n",
        "        self.modelA.fc = nn.Identity()\n",
        "        self.modelB.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.modelA.extract_features(x1)\n",
        "        x2 = self.modelB.extract_features(x2)\n",
        "        \n",
        "        x1 = nn.AdaptiveAvgPool2d((1, 1))(x1)\n",
        "        x2 = nn.AdaptiveAvgPool2d((1, 1))(x2)\n",
        "        x1 = x1.view(1,1536)\n",
        "        x1 = torch.Tensor.cpu(x1).detach().numpy()\n",
        "        x2 = x2.view(1,1536)\n",
        "        x2 = torch.Tensor.cpu(x2).detach().numpy()\n",
        "        x = x1+x2        \n",
        "        sigmoid = tf.math.sigmoid(x)\n",
        "        multi = tf.math.multiply(sigmoid,x2)\n",
        "        add = multi + x1\n",
        "        add = add.numpy()\n",
        "        x = torch.from_numpy(add)\n",
        "        x = self.drop(x)\n",
        "        x = x.flatten()\n",
        "        x = self.out2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGjumvJTXQaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "5cc84ea1-4d07-4ebc-a60c-ed3c5ef9dc62"
      },
      "source": [
        "probabilities = []\n",
        "x_model = NeuralNet(modelA, modelB)\n",
        "y = x_model(input1,input2)\n",
        "print(y)\n",
        "print(type(y))\n",
        "y = torch.Tensor.cpu(y).detach().numpy()\n",
        "print(y)\n",
        "print(type(y))\n",
        "probabilities = softmax(torch.from_numpy(y),dim=0).numpy()\n",
        "print(probabilities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.1252,  0.0514, -0.0458, -0.1257,  0.1552,  0.0167,  0.0959, -0.2829],\n",
            "       grad_fn=<AddBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "[ 0.12519808  0.05135238 -0.04580437 -0.12567937  0.15521011  0.01673217\n",
            "  0.09585901 -0.2829435 ]\n",
            "<class 'numpy.ndarray'>\n",
            "[0.14057976 0.13057259 0.11848336 0.1093876  0.14486279 0.1261295\n",
            " 0.13651519 0.09346933]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fg8-Px4IXoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_model = NeuralNet(modelA, modelB)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-bLyTHt0Woc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_model.load_state_dict(torch.load(os.path.join(\n",
        "        '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/concate/model_concate_epoch5.pth')))\n",
        "x_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8V9yTS0Yp6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98097d77-b43f-408a-f7a4-314cbaba6c64"
      },
      "source": [
        "\n",
        "#optimizer = torch.optim.Adam(x_model.parameters(), lr=1e-5)\n",
        "optimizer = torch.optim.Adam(x_model.parameters(), lr=3e-4)\n",
        "#weights = torch.tensor([0.5, 0.6, 0.7, 0.7, 0.7, 0.7,0.7, 0.6])\n",
        "#weights = weights.cuda()\n",
        "#criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.75, patience=2)\n",
        "scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5, patience=5)\n",
        "\n",
        "dem = 0\n",
        "loss_log = []\n",
        "PATH_SAVE = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/concate_fromepoch5_lr/'\n",
        "if not os.path.exists(PATH_SAVE):\n",
        "       os.makedirs(PATH_SAVE)\n",
        "for epoch in range(20):\n",
        "  print('====Epoch ',epoch,\": \")\n",
        "  for i1, i2 in zip(enumerate(train_loader1),enumerate(train_loader2)):\n",
        "    dem = dem + 1\n",
        "    #print(i1)\n",
        "    stt1, data1 = i1\n",
        "    input1,target1 = data1\n",
        "    #print(target1)\n",
        "    #print(i2)\n",
        "    stt2, data2 = i2\n",
        "    input2,target2 = data2\n",
        "    #print(target2)\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    input1 = input1.cuda()\n",
        "    input2 = input2.cuda()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = x_model(input1,input2)\n",
        "    #print(outputs)\n",
        "    #print(target1)\n",
        "    outputs = outputs.view(1,8)\n",
        "    loss = criterion(outputs, target1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if dem%1000==0:\n",
        "        print('so luong: ',dem,'/',len(train_loader1))\n",
        "  loss_log.append(loss)\n",
        "  print(loss)\n",
        "  dem = 0\n",
        "  torch.save(x_model.state_dict(), PATH_SAVE + 'model_concate_fromepoch5_lr_epoch_'+str(epoch)+'.pth')\n",
        "\n",
        "with open(PATH_SAVE+\"model_concate_fromepoch5_20epoch_lr.txt\", \"w\") as output:\n",
        "    output.write(str(loss_log))\n",
        "    \n",
        "#PATH_SAVE = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/concate/'\n",
        "#if not os.path.exists(PATH_SAVE):\n",
        "#        os.makedirs(PATH_SAVE)\n",
        "#torch.save(x_model.state_dict(), PATH_SAVE + 'model_concate_30epoch.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====Epoch  0 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(11.0529, grad_fn=<NllLossBackward>)\n",
            "====Epoch  1 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(7.4027, grad_fn=<NllLossBackward>)\n",
            "====Epoch  2 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(8.6225, grad_fn=<NllLossBackward>)\n",
            "====Epoch  3 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(8.7669, grad_fn=<NllLossBackward>)\n",
            "====Epoch  4 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(9.2450, grad_fn=<NllLossBackward>)\n",
            "====Epoch  5 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(8.8994, grad_fn=<NllLossBackward>)\n",
            "====Epoch  6 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(6.8826, grad_fn=<NllLossBackward>)\n",
            "====Epoch  7 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(6.5756, grad_fn=<NllLossBackward>)\n",
            "====Epoch  8 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(6.7446, grad_fn=<NllLossBackward>)\n",
            "====Epoch  9 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(8.4033, grad_fn=<NllLossBackward>)\n",
            "====Epoch  10 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(5.6238, grad_fn=<NllLossBackward>)\n",
            "====Epoch  11 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(7.0529, grad_fn=<NllLossBackward>)\n",
            "====Epoch  12 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(5.0186, grad_fn=<NllLossBackward>)\n",
            "====Epoch  13 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(4.4520, grad_fn=<NllLossBackward>)\n",
            "====Epoch  14 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(8.4762, grad_fn=<NllLossBackward>)\n",
            "====Epoch  15 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(7.3364, grad_fn=<NllLossBackward>)\n",
            "====Epoch  16 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(6.1461, grad_fn=<NllLossBackward>)\n",
            "====Epoch  17 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(7.5853, grad_fn=<NllLossBackward>)\n",
            "====Epoch  18 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(5.1024, grad_fn=<NllLossBackward>)\n",
            "====Epoch  19 : \n",
            "so luong:  1000 / 6962\n",
            "so luong:  2000 / 6962\n",
            "so luong:  3000 / 6962\n",
            "so luong:  4000 / 6962\n",
            "so luong:  5000 / 6962\n",
            "so luong:  6000 / 6962\n",
            "tensor(4.3590, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F51fct9Jd5L7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fd6e07-2293-47ff-8d61-d03f8f43c72e"
      },
      "source": [
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9c7d9083178b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAOQIKB3o0fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"model_concate_19epoch.txt\", \"w\") as output:\n",
        "    output.write(str(loss_log))\n",
        "    \n",
        "PATH_SAVE = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/concate/'\n",
        "if not os.path.exists(PATH_SAVE):\n",
        "        os.makedirs(PATH_SAVE)\n",
        "torch.save(x_model.state_dict(), PATH_SAVE + 'model_concate_19epoch.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfYQac_ifO8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_SAVE = '/content/gdrive/My Drive/Colab Notebooks/odir-ivashnyov/ODIR/logs/concate/'\n",
        "if not os.path.exists(PATH_SAVE):\n",
        "        os.makedirs(PATH_SAVE)\n",
        "torch.save(x_model.state_dict(), PATH_SAVE + 'model_concate_epoch10.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXDxnmg4fXpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}